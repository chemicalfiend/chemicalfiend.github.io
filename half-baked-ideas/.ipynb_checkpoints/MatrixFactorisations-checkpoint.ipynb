{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f05984-50a5-4fea-8357-e3b5586755b7",
   "metadata": {},
   "source": [
    "Made by Pranay Venkatesh, April 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a65b55-05df-4520-a0e8-7dbba43083ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076aabf9-0852-4968-8f85-358067c5675b",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition\n",
    "\n",
    "- SVD is a handy linear algebra decomposition that I learned by watching Gilbert Strang the summer before I started undergrad since I was bored and I had heard that it was useful to learn linear algebra.\n",
    "- An SVD is at some level a generalisation of the eigenvalue decomposition. Recall : the eigenvalue decomposition for a Hermitian matrix A was $A = U \\Lambda U^{\\dagger}$ where $U$ is the unitary matrix of eigenvectors and $\\Lambda$ is a diagonal matrix of eigenvalues. If A was instead **not** a Hermitian or even a square matrix, then this doesn't really apply, so we can instead factorise as $A = U \\Sigma V^{\\dagger}$.\n",
    "- Now that A needn't be a square matrix, U and V have different dimensions. $U$ is a matrix that happens to contains the eigenvectors of the Hermitian matrix $A A^{\\dagger}$ and $V$ is a matrix that happens to contain the eigenvectors of the Hermitian matrix $A^{\\dagger} A$\n",
    "- $\\Sigma$ is a diagonal matrix of what are called \"singular values\" for this system. Note that $\\Sigma^{\\dagger} \\Sigma$ are the eigenvalues of $A^{\\dagger} A$ and $\\Sigma \\Sigma^{\\dagger}$ are the eigenvalues of $A A^{\\dagger}$. The square of the singular values are the eigenvalues of the matrices $A^{\\dagger} A$ or $A A^{\\dagger}$ (obviously they have different number of eigenvalues but you can capture the non-zero ones easily at least).\n",
    "- From a linear algebra perspective : we can think of the SVD as resulting from trying to construct an orthonormal basis in the rowspace using an orthonormal basis in the columnspace. Stacking the columnspace vectors together is V and stacking the rowspace vectors together is U. You get something similar to an eigenvalue equation. For eigenvalue equations : $A v = \\lambda v$ while for singular value equations you get $A v = \\sigma u$ since A need not be square, v and u can be of different dimensions.\n",
    "- Upto a sign factor on the elements of U and V, the SVD is unique for a matrix A.\n",
    "- If A is a Hermitian positive-definite matrix, then the SVD reduces to an eigenvalue decomposition.\n",
    "  \n",
    "These are all fairly easy to describe with a Julia code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f129a1e8-baac-4c08-b65d-25e2418795de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVD{Float64, Float64, Matrix{Float64}, Vector{Float64}}\n",
       "U factor:\n",
       "4×4 Matrix{Float64}:\n",
       " 0.0       1.0   0.0        0.0\n",
       " 0.375623  0.0  -0.554241   0.742781\n",
       " 0.919925  0.0   0.125726  -0.371391\n",
       " 0.112453  0.0   0.822806   0.557086\n",
       "singular values:\n",
       "4-element Vector{Float64}:\n",
       " 7.285821103869117\n",
       " 2.23606797749979\n",
       " 2.217388293108676\n",
       " 0.0\n",
       "Vt factor:\n",
       "4×5 Matrix{Float64}:\n",
       " -0.0       0.409656   0.91224   -0.0  0.0\n",
       "  0.447214  0.0        0.0        0.0  0.894427\n",
       " -0.0       0.91224   -0.409656  -0.0  0.0\n",
       "  0.0       0.0        0.0        1.0  0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 3. 6. 0. 0.; 0. 2. 0. 0. 0.]\n",
    "F = svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb1299e3-7030-48ba-bbf5-bc4810009158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 1.0   0.0   0.0  0.0  2.0\n",
       " 0.0  13.0  18.0  0.0  0.0\n",
       " 0.0  18.0  45.0  0.0  0.0\n",
       " 0.0   0.0   0.0  0.0  0.0\n",
       " 2.0   0.0   0.0  0.0  4.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Aleft = A * transpose(A)\n",
    "Aright = transpose(A) * A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43d74204-c2d8-4a59-8af0-a680599fe21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Eigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\n",
       "values:\n",
       "4-element Vector{Float64}:\n",
       " -3.2656641468096717e-305\n",
       "  4.916810842415401\n",
       "  5.0\n",
       " 53.0831891575846\n",
       "vectors:\n",
       "4×4 Matrix{Float64}:\n",
       "  0.0        0.0       1.0  0.0\n",
       "  0.742781   0.554241  0.0  0.375623\n",
       " -0.371391  -0.125726  0.0  0.919925\n",
       "  0.557086  -0.822806  0.0  0.112453"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen(Aleft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5099dc9-9dc3-4e63-8fbd-383847f04dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Eigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\n",
       "values:\n",
       "5-element Vector{Float64}:\n",
       "  0.0\n",
       "  1.7763568394002505e-15\n",
       "  4.916810842415423\n",
       "  5.0\n",
       " 53.08318915758459\n",
       "vectors:\n",
       "5×5 Matrix{Float64}:\n",
       "  0.0  -0.894427   0.0       0.447214  0.0\n",
       "  0.0   0.0        0.91224   0.0       0.409656\n",
       "  0.0   0.0       -0.409656  0.0       0.91224\n",
       " -1.0   0.0        0.0       0.0       0.0\n",
       "  0.0   0.447214   0.0       0.894427  0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen(Aright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66fcc0f4-ff0a-4a3c-a076-c244acea1369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 53.0831891575846\n",
       "  5.000000000000001\n",
       "  4.9168108424154084\n",
       "  0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(F.S).^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae7d2a0-1083-4cca-8dcd-9cdf7ab2d64f",
   "metadata": {},
   "source": [
    "## Why SVDs?\n",
    "\n",
    "- There are several ways to factorise matrices to use them to do various things, but SVDs are particularly handy since the truncated SVD of rank k is the *best* approximation (of rank k) to the original matrix A. This is called the Eckart-Young theorem.\n",
    "- If $A = U \\Sigma V^{\\dagger} = \\sigma_1 u_1 v_1^{\\dagger} + \\sigma_2 u_2 v_2^{\\dagger} + ... \\sigma_r u_r v_r^{\\dagger}$ is a rank r matrix, then the rank k truncation is given by $A_k = U_k \\Sigma_k V_k^{\\dagger} = \\sigma_1 u_1 v_1^{\\dagger} + \\sigma_2 u_2 v_2^{\\dagger} + ... \\sigma_k u_k v_k^{\\dagger}$ where of course k < r.\n",
    "- **NOTE**: when we make this truncation, we sort the singular values in descending order, so that we only drop terms with the lowest singular values or the lowest importance.\n",
    "- More formally we use the Froebenius norm, defined by $||A||_F = \\sum\\limits_{i, j} |A_{i,j}|^2$ to say that given a matrix A and the SVD representation of rank k given by $A_k$, then any matrix B of rank k would have the property $||A - A_k||_F \\leq ||A - B||_F$ or that $A_k$ is the closer to $A$ than any matrix B of rank k.\n",
    "- This is nice to look at with a julia code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da7bcb22-c1e4-4cd3-811e-314c59b9d2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2173882931086766"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVD truncation \n",
    "\n",
    "function tsvd(A::Matrix, k::Int)\n",
    "    F = svd(A)\n",
    "    U, S, V = F.U, F.S, F.V\n",
    "    U_k = U[:, 1:k]\n",
    "    S_k = S[1:k]\n",
    "    Vt_k = V[:, 1:k]'\n",
    "    A_k = U_k * Diagonal(S_k) * Vt_k\n",
    "    \n",
    "    return U_k, S_k, Vt_k, A_k\n",
    "end\n",
    "\n",
    "u, s, v, a = tsvd(A, 2) # Rank 2 truncation\n",
    "\n",
    "norm(A - a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b255ed34-c8a4-47e3-bc81-bdc6f3805a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.510131840322714"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = randn(4, 2)\n",
    "V = randn(2, 5)\n",
    "\n",
    "B = U * V\n",
    "print(rank(B))\n",
    "norm(A - B, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a8ad74-dc49-454d-8737-e0c3d9ebd804",
   "metadata": {},
   "source": [
    "You can keep running the last block of code and you will find that the norm(A - B) will never go above norm(A - a). Note that here B is constructed to be a rank 2 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a99ab-d124-4c6d-b5c4-5b484a78fddc",
   "metadata": {},
   "source": [
    "SVD is particularly handy for the construction and propagation of **matrix product states** (MPS) which is what led me to come back and revisit some of the beautiful matrix math that underlies these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4450241-cf8d-4290-9728-b3b5a8ab867b",
   "metadata": {},
   "source": [
    "# QR Decomposition\n",
    "\n",
    "- The QR decomposition is another matrix factorisation that I learned from Gilbert Strang rather than my (highly inadequate) undergraduate linear algebra class.\n",
    "- Any square matrix A can be decomposed into Q times R where Q is an orthogonal matrix and R is an upper triangular matrix. So why would we want to do this? Because this is a useful way to compute the eigenvalues for a hermitian matrix.\n",
    "- Start by noting : If you write $A_0 = Q_0 R_0$ and then you get a new matrix $A_1 = R_0 Q_0$ (order swapped), then $A_1$ and $A_0$ have the same eigenvalues since $A_1 = R_0 A_0 R_0^{-1}$ (they are related by a similarity).\n",
    "- If you keep repeating this procedure of QR decomposition followed by swapping the matrix orders, it turns out that the diagonal elements of $A_n$ become the eigenvalues pretty quickly since the off-diagonals become small (very fast).\n",
    "- QR is faster on GPUs than SVDs and hence you can also do a QR decomposition when using MPS (there's a Frank Pollman paper on Arxiv that does this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf4eebb1-062e-4cf7-9cf7-2de8080d37ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearAlgebra.QRCompactWY{Float64, Matrix{Float64}, Matrix{Float64}}\n",
       "Q factor: 4×4 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\n",
       "R factor:\n",
       "4×5 Matrix{Float64}:\n",
       " 1.0   0.0       0.0      0.0  2.0\n",
       " 0.0  -3.60555  -4.9923   0.0  0.0\n",
       " 0.0   0.0       4.48073  0.0  0.0\n",
       " 0.0   0.0       0.0      0.0  0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qr(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7a8b5-66de-46e7-9843-9e3172008dc5",
   "metadata": {},
   "source": [
    "Relevant Reading: \n",
    "1. Strang 18.06, 18.065 (more computational : this is the course that got me introduced to [julia](https://chemicalfiend.github.io/half-baked-ideas/julia.html)\n",
    "2. Numerical Recipes in C : goldmine for computational things\n",
    "3. [QR in MPS](https://arxiv.org/pdf/2212.09782)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5dbe65-ea3d-4432-acf8-eac951517e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
